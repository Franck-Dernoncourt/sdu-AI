# SDU - Acronym Identification

This repository contains the acronym identification training and development set along with the evaluation scropts for acronym identification task at SDU@AAAI-21.

# Dataset

The dataset folder contains three files:

- **train.json**: The training samples for acronym identification task. Each sample has three attributes:
  - tokens: The list of words (tokens) of the sample
  - labels: The short-form and long-form labels of the words in BIO format. The labels `B-short` and `B-long` identifies the begining of a short-form and long-form phrase, respectively. The labels `I-short` and `I-long` indicates the words inside the short-form or long-form phrases. Finally, the label `O` shows the word is not part of any short-form or long-form phrase. 
  - id: The unique ID of the sample
- **dev.json**: The development set for acronym identification task. The samples in `dev.json` have the same attributes as the samples in `train.json`.
- **predictions.json**: A sample prediction file created from `dev.json` to test the scoring script. The participants should submit the final test predictions of their model in the same format as the `predictions.json` file. Each prediction should have two attributes:
  - id: The ID of the sample (i.e., the same IDs used in the train/dev/test samples provided in `train.json`, `dev.json` and `test.json`) 
  - predictions: The labels of the words of the sample in BIO format. The labels `B-short` and `B-long` identifies the begining of a short-form and long-form phrase, respectively. The labels `I-short` and `I-long` indicates the words inside the short-form or long-form phrases. Finally, the label `O` shows the word is not part of any short-form or long-form phrase.
  
  
# Evaluation

To evaluate the predictions (in the format provided in `dataset/predictions.json` file), run the following command:

`python scorere.py -g path/to/gold.json -p path/to/predictions.json`

The `path/to/gold.json` and `path/to/predictions.json` should be replaced with the real paths to the gold file (e.g., `dataset/dev.json` for evaluation on development set) and predictions file (i.e., the predictions generated by your system in the same format as `dataset/predictions.json` file). The official evaluation metrics are the macro-averaged precision, recall and F1 for short form and long form predictions. For verbose evaluation (including the micro-averaged precision, recall and F1 and also short form and long form scores seperatedly), use the following command:

`python scorere.py -g path/to/gold.json -p path/to/predictions.json -v`

# Submission

Submit the predictions of the model for the samples of the test set, which will be provided later, to [EasyChair](https://urldefense.com/v3/__https://easychair.org/conferences/?conf=sduaaai21__;!!C5qS4YX3!Sgxkhh2juEB5WzmclunaUhWV76hQBFnIc9fVz_658mfwcw6DvfoXu6GqUHOE3AQKYA$). Please note that the submitted prediction file should have the same format as the `datasset/predictions.json` file and use the IDs of the test samples. 

For more information see [SDU@AAAI-21](https://sites.google.com/view/sdu-aaai21/home)
